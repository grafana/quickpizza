// discover containers running QuickPizza
discovery.docker "application_containers" {
  host = "unix:///var/run/docker.sock"
  filter {
    name   = "label"
    values = ["service.type=application"]
  }
}

// set expected labels
discovery.relabel "application_containers" {
  // Application Observability expects `job` to be `$service.namespace/$service.name`
  // Reads from service.namespace Docker label and container name to build: namespace/service-name
  rule {
    target_label = "job"
    source_labels = [
      "__meta_docker_container_label_service_namespace",
      "__meta_docker_container_name",
    ]
    regex = "([^;]+);/(.*)"
    replacement = "${1}/${2}"
  }
  rule {
    target_label = "instance"
    source_labels = [
      "__meta_docker_container_name",
    ]
    regex = "/(.*)"
    replacement = "${1}"
  }
  rule {
    target_label = "service_namespace"
    source_labels = [
      "__meta_docker_container_label_service_namespace",
    ]
  }
  // the `namespace` label is for visualizing Profiles in Application Observability
  rule {
    target_label = "namespace"
    source_labels = [
      "__meta_docker_container_label_service_namespace",
    ]
  }
  rule {
    target_label = "service_name"
    source_labels = [
      "__meta_docker_container_name",
    ]
    regex = "/(.*)"
    replacement = "${1}"
  }
  rule {
    target_label = "deployment_environment"
    replacement = coalesce(env("DEPLOYMENT_ENVIRONMENT"), "development")
  }
  targets = discovery.docker.application_containers.targets
}

// Metrics
prometheus.remote_write "local" {
  endpoint {
    // TODO: Replace this with your prometheus-compatible metrics store
    url = env("METRICS_ENDPOINT")
    send_native_histograms = true
  }
}
prometheus.scrape "application_containers" {
  scrape_interval = "10s"
  targets = discovery.relabel.application_containers.output
  forward_to = [prometheus.remote_write.local.receiver]
}

// Logs
loki.write "local" {
  endpoint {
    url = env("LOGS_ENDPOINT")
  }
}
loki.source.docker "application_containers" {
  host       = "unix:///var/run/docker.sock"
  targets    = discovery.relabel.application_containers.output
  forward_to = [loki.write.local.receiver]
}

// Profiling Pull Mode
pyroscope.write "local" {
   endpoint {
       url = env("PROFILES_ENDPOINT")
   }
}
discovery.relabel "application_containers_profiles" {
  // Filter to only scrape port 3333 for profiling.
  // Docker discovery creates separate targets for each exposed port (3333, 3334, 3335).
  // We only keep port 3333 where the HTTP pprof endpoints are available.
  // Ports 3334 and 3335 are gRPC servers without pprof Pull support.
  rule {
    source_labels = ["__meta_docker_port_private"]
    regex         = "3333"
    action        = "keep"
  }
  // https://grafana.com/docs/pyroscope/latest/view-and-analyze-profile-data/line-by-line/
  rule {
    target_label = "service_repository"
    replacement = coalesce(env("QUICKPIZZA_PYROSCOPE_SERVICE_REPOSITORY"), "https://github.com/grafana/quickpizza")
  }
  rule {
    target_label = "service_git_ref"
    replacement = coalesce(env("QUICKPIZZA_PYROSCOPE_SERVICE_GIT_REF"), "refs/heads/main")
  }
  targets = discovery.relabel.application_containers.output
}
pyroscope.scrape "application_containers" {
  // https://grafana.com/docs/pyroscope/latest/configure-client/grafana-alloy/go_pull/
  scrape_interval = "30s"
  targets = discovery.relabel.application_containers_profiles.output
  forward_to = [pyroscope.write.local.receiver]
}


// Receive traces and metrics with OTEL setup
otelcol.receiver.otlp "default" {
  // configures the default grpc endpoint "0.0.0.0:4317"
  grpc {
    endpoint = "0.0.0.0:4317"
  }

  // configures the default http/protobuf endpoint "0.0.0.0:4318"
  http {
    endpoint = "0.0.0.0:4318"
  }

  output {
    // Receive metrics
    metrics = [otelcol.processor.batch.default.input]
    traces  = [
               // Traces to ServiceGraph metrics
               otelcol.connector.servicegraph.default.input,
               // Traces to SpanMetrics
               otelcol.processor.transform.spanmetrics.input,
               // Traces to OTLP endpoint
               otelcol.processor.batch.default.input,
               ]
  }
}

// Traces to span metrics
// https://grafana.com/docs/tempo/latest/metrics-from-traces/span-metrics/span-metrics-alloy/

// Remove all resource attributes except the ones the otelcol.connector.spanmetrics needs.
otelcol.processor.transform "spanmetrics" {
  error_mode = "ignore"

  trace_statements {
    context = "resource"
    statements = [
      // We keep only the "service.name" and "special.attr" resource attributes,
      // because they are the only ones which otelcol.connector.spanmetrics needs.
      //
      // There is no need to list "span.name", "span.kind", and "status.code"
      // here because they are properties of the span (and not resource attributes):
      // https://github.com/open-telemetry/opentelemetry-proto/blob/v1.0.0/opentelemetry/proto/trace/v1/trace.proto
      `keep_keys(attributes, ["service.name", "special.attr"])`,
    ]
  }
  output {
    traces  = [otelcol.connector.spanmetrics.default.input]
  }
}
otelcol.connector.spanmetrics "default" {
  metrics_flush_interval = "10s"

  histogram {
    // `s` is the unit used by Application Observability
    // https://grafana.com/docs/grafana-cloud/monitor-applications/application-observability/setup/metrics-labels/
    // this creates `traces_span_metrics_duration_seconds` instead of `traces_span_metrics_duration_milliseconds`
    unit = "s"
    // enable `exponential` option to send native histogram metrics
    exponential {
      max_size = 160
    }
    // `explicit` converts to classic histograms
    // explicit { buckets = ["50ms", "100ms", "250ms", "1s", "5s", "10s"]}
  }
  dimension {
    name = "special.attr"
  }

  exemplars {
    enabled = true
  }

  output {
    metrics = [otelcol.processor.batch.default.input]
  }
}

// Traces to servicegraph metrics
// https://grafana.com/docs/tempo/latest/metrics-from-traces/service_graphs/enable-service-graphs/
otelcol.connector.servicegraph "default" {
  metrics_flush_interval = "10s"
  dimensions = ["http.method", "http.target"]
  // TODO:  Enable when Alloy supports `exponential_histogram_max_size`
  // enable `exponential_histogram_max_size` option to send native histogram metrics
  // exponential_histogram_max_size = 16
  output {
    metrics = [otelcol.processor.batch.default.input]
  }
}
// Traces to OTLP endpoint and Metrics to Prometheus
otelcol.processor.batch "default" {
  output {
    metrics = [otelcol.exporter.prometheus.local.input]
    logs = []
    traces = [
      otelcol.exporter.otlp.default.input,
    ]
  }
}

otelcol.exporter.prometheus "local" {
  forward_to = [prometheus.remote_write.local.receiver]
}
otelcol.exporter.otlp "default" {
  client {
    // TODO: Replace this with the endpoint for your trace receiver
    endpoint = env("TRACES_ENDPOINT")
    tls {
        insecure             = true
        insecure_skip_verify = true
    }
  }
}